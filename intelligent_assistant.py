#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ò—Å—Ç–∏–Ω–Ω–æ –ø—Ä–æ–∞–∫—Ç–∏–≤–Ω—ã–π –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –í–°–ï –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∑–Ω–∞–Ω–∏–π:
- –î–æ–∫—É–º–µ–Ω—Ç—ã –≤ Qdrant
- –î–∞—Ç–∞—Å–µ—Ç Q&A —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞
–ò –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —É—Ç–æ—á–Ω—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã
"""

import logging
import torch
import re
import json
from typing import Dict, List, Optional, Tuple, Set
from transformers import AutoModelForCausalLM, AutoTokenizer
from ingest import client as qdrant, COLLECTION, embedder
import pathlib

log = logging.getLogger(__name__)

class EnhancedIntelligentProactiveAssistant:
    """–£–º–Ω—ã–π –ø—Ä–æ–∞–∫—Ç–∏–≤–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å –∞–Ω–∞–ª–∏–∑–æ–º –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∑–Ω–∞–Ω–∏–π"""
    
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.knowledge_cache = {}
        self.qa_dataset = self._load_qa_dataset()
        
    def _load_qa_dataset(self) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç Q&A —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞"""
        try:
            dataset_path = pathlib.Path("developer_dataset.json")
            if dataset_path.exists():
                with open(dataset_path, 'r', encoding='utf-8') as f:
                    dataset = json.load(f)
                    log.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –¥–∞—Ç–∞—Å–µ—Ç Q&A: {len(dataset)} –ø–∞—Ä")
                    return dataset
            else:
                log.info("‚ÑπÔ∏è  –î–∞—Ç–∞—Å–µ—Ç Q&A –Ω–µ –Ω–∞–π–¥–µ–Ω")
                return []
        except Exception as e:
            log.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ Q&A: {e}")
            return []
    
    def reload_qa_dataset(self):
        """–ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç Q&A (–¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏)"""
        old_count = len(self.qa_dataset)
        self.qa_dataset = self._load_qa_dataset()
        new_count = len(self.qa_dataset)
        if new_count != old_count:
            log.info(f"üîÑ –î–∞—Ç–∞—Å–µ—Ç Q&A –æ–±–Ω–æ–≤–ª–µ–Ω: {old_count} -> {new_count} –∑–∞–ø–∏—Å–µ–π")

    def analyze_comprehensive_knowledge(self, user_query: str) -> Dict:
        """
        –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ –í–°–ï–ú –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º –∑–Ω–∞–Ω–∏–π:
        1. –î–æ–∫—É–º–µ–Ω—Ç—ã –≤ Qdrant
        2. –î–∞—Ç–∞—Å–µ—Ç Q&A —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞
        """
        log.info(f"üîç –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞: {user_query}")
        
        try:
            # –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏
            self.reload_qa_dataset()
            
            # 1. –ü–æ–∏—Å–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö Qdrant
            qdrant_results = self._search_qdrant_knowledge(user_query)
            
            # 2. –ü–æ–∏—Å–∫ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ Q&A
            qa_results = self._search_qa_dataset(user_query)
            
            # 3. –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            combined_analysis = self._combine_knowledge_sources(
                user_query, qdrant_results, qa_results
            )
            
            # 4. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            missing_info = self._identify_comprehensive_missing_info(
                combined_analysis, user_query
            )
            
            # 5. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–º–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            smart_questions = self._generate_comprehensive_questions(
                user_query, combined_analysis, missing_info
            )
            
            return {
                "status": "comprehensive_analysis_complete",
                "questions": smart_questions,
                "qdrant_context": qdrant_results.get("context", ""),
                "qa_context": qa_results.get("context", ""),
                "combined_context": combined_analysis["full_context"],
                "missing_info": missing_info,
                "confidence": combined_analysis["confidence"],
                "sources": {
                    "qdrant_sources": qdrant_results.get("sources_count", 0),
                    "qa_matches": qa_results.get("matches_count", 0),
                    "total_sources": qdrant_results.get("sources_count", 0) + qa_results.get("matches_count", 0)
                }
            }
            
        except Exception as e:
            log.error(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return {"status": "error", "questions": [], "context": str(e)}
    
    def _search_qdrant_knowledge(self, query: str) -> Dict:
        """–ü–æ–∏—Å–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö Qdrant"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            collections = qdrant.get_collections().collections
            if COLLECTION not in {c.name for c in collections}:
                return {"context": "", "sources_count": 0, "confidence": 0}
            
            # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∏ –ø–æ–∏—Å–∫
            query_vector = list(embedder.embed([query]))[0]
            hits = qdrant.search(
                collection_name=COLLECTION,
                query_vector=query_vector,
                limit=10,  # –ò—â–µ–º –±–æ–ª—å—à–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                with_payload=True,
                score_threshold=0.3,
            )
            
            if not hits:
                return {"context": "", "sources_count": 0, "confidence": 0}
            
            # –ê–Ω–∞–ª–∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            documents_context = []
            sources = set()
            procedures = []
            required_fields = set()
            
            for hit in hits[:5]:  # –ë–µ—Ä–µ–º —Ç–æ–ø-5
                text = hit.payload.get("text", "")
                source = hit.payload.get("file", "unknown")
                
                documents_context.append(f"[{source}] {text}")
                sources.add(source)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–æ—Ü–µ–¥—É—Ä–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                self._extract_procedure_info(text, procedures, required_fields)
            
            context = "\n\n".join(documents_context)
            confidence = max(hit.score for hit in hits) if hits else 0
            
            return {
                "context": context,
                "sources_count": len(sources),
                "confidence": confidence,
                "procedures": procedures[:5],
                "required_fields": list(required_fields)[:10],
                "hit_scores": [hit.score for hit in hits]
            }
            
        except Exception as e:
            log.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ Qdrant: {e}")
            return {"context": "", "sources_count": 0, "confidence": 0}
    
    def _search_qa_dataset(self, query: str) -> Dict:
        """–ü–æ–∏—Å–∫ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ Q&A —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞"""
        if not self.qa_dataset:
            return {"context": "", "matches_count": 0, "confidence": 0}
        
        try:
            query_clean = self._clean_text_for_matching(query)
            matches = []
            
            for qa_pair in self.qa_dataset:
                question = qa_pair.get("question", "")
                answer = qa_pair.get("answer", "")
                
                # –í—ã—á–∏—Å–ª—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
                question_similarity = self._calculate_text_similarity(
                    query_clean, self._clean_text_for_matching(question)
                )
                
                answer_similarity = self._calculate_text_similarity(
                    query_clean, self._clean_text_for_matching(answer)
                )
                
                # –ë–µ—Ä–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
                max_similarity = max(question_similarity, answer_similarity)
                
                if max_similarity > 0.2:  # –ü–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                    matches.append({
                        "question": question,
                        "answer": answer,
                        "similarity": max_similarity,
                        "source": "developer_qa"
                    })
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            matches.sort(key=lambda x: x["similarity"], reverse=True)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –ª—É—á—à–∏—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
            context_parts = []
            for match in matches[:3]:  # –¢–æ–ø-3 —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                context_parts.append(
                    f"[Q&A] –í–æ–ø—Ä–æ—Å: {match['question']}\n–û—Ç–≤–µ—Ç: {match['answer']}"
                )
            
            context = "\n\n".join(context_parts)
            confidence = matches[0]["similarity"] if matches else 0
            
            return {
                "context": context,
                "matches_count": len(matches),
                "confidence": confidence,
                "best_matches": matches[:3],
                "all_similarities": [m["similarity"] for m in matches]
            }
            
        except Exception as e:
            log.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ Q&A: {e}")
            return {"context": "", "matches_count": 0, "confidence": 0}
    
    def _clean_text_for_matching(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è"""
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text = text.lower().strip()
        # –£–¥–∞–ª—è–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è, –∫—Ä–æ–º–µ –≤–∞–∂–Ω—ã—Ö
        text = re.sub(r'[^\w\s‚Ññ\-]', '', text)
        # –£–¥–∞–ª—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\s+', ' ', text)
        return text
    
    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ö–æ–∂–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–æ–≤"""
        words1 = set(text1.split())
        words2 = set(text2.split())
        
        if not words1 or not words2:
            return 0.0
        
        # 1. –°—Ö–æ–∂–µ—Å—Ç—å –ñ–∞–∫–∫–∞—Ä–∞
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        jaccard = len(intersection) / len(union) if union else 0.0
        
        # 2. –ë–æ–Ω—É—Å –∑–∞ —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
        key_terms = {"vlife", "–±–æ–Ω—É—Å", "–∞–∑—Å", "–æ–ø–ª–∞—Ç–∞", "–∫–∞—Ä—Ç–∞", "—Ç–æ–ø–ª–∏–≤–æ", "–ø—Ä–æ–±–ª–µ–º–∞"}
        key_matches = words1.intersection(words2).intersection(key_terms)
        key_bonus = len(key_matches) * 0.1
        
        # 3. –ë–æ–Ω—É—Å –∑–∞ –¥–ª–∏–Ω–Ω—ã–µ —Å–æ–≤–ø–∞–¥–∞—é—â–∏–µ —Ñ—Ä–∞–∑—ã
        phrase_bonus = self._calculate_phrase_similarity(text1, text2)
        
        # 4. –ë–æ–Ω—É—Å –∑–∞ —á–∏—Å–ª–æ–≤—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è (–Ω–æ–º–µ—Ä–∞ –ê–ó–° –∏ —Ç.–¥.)
        number_bonus = self._calculate_number_similarity(text1, text2)
        
        total_score = jaccard + key_bonus + phrase_bonus + number_bonus
        return min(total_score, 1.0)
    
    def _calculate_phrase_similarity(self, text1: str, text2: str) -> float:
        """–ë–æ–Ω—É—Å –∑–∞ —Å–æ–≤–ø–∞–¥–∞—é—â–∏–µ —Ñ—Ä–∞–∑—ã"""
        words1 = text1.split()
        words2 = text2.split()
        max_phrase_len = 0
        
        for i in range(len(words1)):
            for j in range(len(words2)):
                if words1[i] == words2[j]:
                    phrase_len = 1
                    k, l = i + 1, j + 1
                    while (k < len(words1) and l < len(words2) and 
                           words1[k] == words2[l]):
                        phrase_len += 1
                        k += 1
                        l += 1
                    max_phrase_len = max(max_phrase_len, phrase_len)
        
        return min(max_phrase_len * 0.05, 0.15)
    
    def _calculate_number_similarity(self, text1: str, text2: str) -> float:
        """–ë–æ–Ω—É—Å –∑–∞ —Å–æ–≤–ø–∞–¥–∞—é—â–∏–µ –Ω–æ–º–µ—Ä–∞"""
        numbers1 = set(re.findall(r'\b\d+\b', text1))
        numbers2 = set(re.findall(r'\b\d+\b', text2))
        
        if numbers1 and numbers2:
            common_numbers = numbers1.intersection(numbers2)
            return min(len(common_numbers) * 0.1, 0.2)
        return 0
    
    def _combine_knowledge_sources(self, query: str, qdrant_results: Dict, qa_results: Dict) -> Dict:
        """–ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        contexts = []
        
        if qdrant_results.get("context"):
            contexts.append(f"=== –î–û–ö–£–ú–ï–ù–¢–´ ===\n{qdrant_results['context']}")
        
        if qa_results.get("context"):
            contexts.append(f"=== –ë–ê–ó–ê –ó–ù–ê–ù–ò–ô Q&A ===\n{qa_results['context']}")
        
        full_context = "\n\n".join(contexts)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â—É—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        qdrant_confidence = qdrant_results.get("confidence", 0)
        qa_confidence = qa_results.get("confidence", 0)
        
        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        if qdrant_confidence > 0 and qa_confidence > 0:
            combined_confidence = (qdrant_confidence + qa_confidence) / 2
        else:
            combined_confidence = max(qdrant_confidence, qa_confidence)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ–º–∏–Ω–∏—Ä—É—é—â–∏–π –∏—Å—Ç–æ—á–Ω–∏–∫
        dominant_source = "hybrid"
        if qdrant_confidence > qa_confidence * 1.5:
            dominant_source = "documents"
        elif qa_confidence > qdrant_confidence * 1.5:
            dominant_source = "qa_dataset"
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã –∏ –ø–æ–ª—è
        all_procedures = qdrant_results.get("procedures", [])
        all_required_fields = qdrant_results.get("required_fields", [])
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ Q&A
        qa_procedures, qa_fields = self._extract_qa_procedures(qa_results)
        all_procedures.extend(qa_procedures)
        all_required_fields.extend(qa_fields)
        
        return {
            "full_context": full_context,
            "confidence": combined_confidence,
            "dominant_source": dominant_source,
            "all_procedures": list(set(all_procedures))[:7],
            "all_required_fields": list(set(all_required_fields))[:10],
            "source_breakdown": {
                "qdrant_confidence": qdrant_confidence,
                "qa_confidence": qa_confidence,
                "qdrant_sources": qdrant_results.get("sources_count", 0),
                "qa_matches": qa_results.get("matches_count", 0)
            }
        }
    
    def _extract_qa_procedures(self, qa_results: Dict) -> Tuple[List[str], List[str]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–æ—Ü–µ–¥—É—Ä—ã –∏ –ø–æ–ª—è –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Q&A"""
        procedures = []
        fields = []
        
        for match in qa_results.get("best_matches", []):
            answer = match.get("answer", "")
            
            # –ò—â–µ–º –ø–æ—à–∞–≥–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≤ –æ—Ç–≤–µ—Ç–∞—Ö
            steps = re.findall(r'\d+\.\s+([^.]+)', answer)
            procedures.extend(steps[:2])
            
            # –ò—â–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
            field_patterns = [
                r'[–ù–Ω]–æ–º–µ—Ä\s+([^.]+)',
                r'[–î–¥]–∞—Ç–∞\s+([^.]+)',
                r'[–°—Å]—É–º–º–∞\s+([^.]+)',
                r'[–¢—Ç]–µ–ª–µ—Ñ–æ–Ω\s+([^.]+)',
            ]
            
            for pattern in field_patterns:
                matches = re.findall(pattern, answer)
                fields.extend(matches[:2])
        
        return procedures, fields
    
    def _extract_procedure_info(self, text: str, procedures: List, fields: Set):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ—Ü–µ–¥—É—Ä–∞—Ö –∏ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª—è—Ö"""
        
        # –ò—â–µ–º –ø–æ—à–∞–≥–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
        step_patterns = [
            r'\d+\.\s+([^.]+)',
            r'[–®—à]–∞–≥\s+\d+[:\.]?\s+([^.]+)',
            r'[–°—Å]–Ω–∞—á–∞–ª–∞\s+([^.]+)',
            r'[–ó–∑]–∞—Ç–µ–º\s+([^.]+)',
            r'[–î–¥]–∞–ª–µ–µ\s+([^.]+)'
        ]
        
        for pattern in step_patterns:
            matches = re.findall(pattern, text)
            procedures.extend(matches[:3])
        
        # –ò—â–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è/—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
        field_patterns = [
            r'[–ù–Ω]—É–∂–Ω–æ|[–¢—Ç]—Ä–µ–±—É–µ—Ç—Å—è|[–£—É]–∫–∞–∂–∏—Ç–µ|[–ü–ø]—Ä–µ–¥–æ—Å—Ç–∞–≤—å—Ç–µ]\s+([^.]+)',
            r'[–ù–Ω]–µ–æ–±—Ö–æ–¥–∏–º–æ\s+([^.]+)',
            r'‚Ññ\s*(\w+)',
            r'[–î–¥]–∞—Ç–∞\s+([^.]+)',
            r'[–í–≤]—Ä–µ–º—è\s+([^.]+)',
            r'[–°—Å]—É–º–º–∞\s+([^.]+)'
        ]
        
        for pattern in field_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches[:5]:
                if len(match.strip()) > 3 and len(match.strip()) < 50:
                    fields.add(match.strip())
    
    def _identify_comprehensive_missing_info(self, analysis: Dict, user_query: str) -> List[str]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω–µ–¥–æ—Å—Ç–∞—é—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        
        missing = []
        query_lower = user_query.lower()
        full_context = analysis["full_context"].lower()
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        info_categories = {
            "location": {
                "keywords": ["–∞–∑—Å", "–∞–¥—Ä–µ—Å", "–Ω–æ–º–µ—Ä", "–≥–¥–µ", "–∑–∞–ø—Ä–∞–≤–∫–∞"],
                "importance": "–≤—ã—Å–æ–∫–∞—è"
            },
            "time": {
                "keywords": ["–∫–æ–≥–¥–∞", "–¥–∞—Ç–∞", "–≤—Ä–µ–º—è", "—á–∞—Å", "–¥–µ–Ω—å"],
                "importance": "–≤—ã—Å–æ–∫–∞—è"
            },
            "amount": {
                "keywords": ["—Å—É–º–º–∞", "–æ–±—ä–µ–º", "–ª–∏—Ç—Ä", "—Ä—É–±–ª", "—Å—Ç–æ–∏–º–æ—Å—Ç—å"],
                "importance": "—Å—Ä–µ–¥–Ω—è—è"
            },
            "contact": {
                "keywords": ["—Ç–µ–ª–µ—Ñ–æ–Ω", "–Ω–æ–º–µ—Ä", "—Å–≤—è–∑—å", "–∫–æ–Ω—Ç–∞–∫—Ç"],
                "importance": "—Å—Ä–µ–¥–Ω—è—è"
            },
            "document": {
                "keywords": ["–¥–æ–∫—É–º–µ–Ω—Ç", "—Å–ø—Ä–∞–≤–∫–∞", "–Ω–æ–º–µ—Ä", "—á–µ–∫"],
                "importance": "–Ω–∏–∑–∫–∞—è"
            },
            "person": {
                "keywords": ["–∫—Ç–æ", "—Ñ–∞–º–∏–ª–∏—è", "–∏–º—è", "—Å–æ—Ç—Ä—É–¥–Ω–∏–∫"],
                "importance": "–Ω–∏–∑–∫–∞—è"
            },
            "details": {
                "keywords": ["–∫–∞–∫ –∏–º–µ–Ω–Ω–æ", "–ø–æ–¥—Ä–æ–±–Ω–æ", "–¥–µ—Ç–∞–ª–∏", "—á—Ç–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ"],
                "importance": "–≤—ã—Å–æ–∫–∞—è"
            },
            "payment_method": {
                "keywords": ["–∫–∞—Ä—Ç–∞", "–Ω–∞–ª–∏—á–Ω—ã–µ", "–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ", "—Ç–µ—Ä–º–∏–Ω–∞–ª"],
                "importance": "—Å—Ä–µ–¥–Ω—è—è"
            },
            "vehicle": {
                "keywords": ["–º–∞—à–∏–Ω–∞", "–∞–≤—Ç–æ–º–æ–±–∏–ª—å", "–º–∞—Ä–∫–∞", "–Ω–æ–º–µ—Ä –∞–≤—Ç–æ"],
                "importance": "–Ω–∏–∑–∫–∞—è"
            }
        }
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏
        query_type = self._determine_query_type(query_lower)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        mentioned_categories = set()
        for category, info in info_categories.items():
            if any(keyword in query_lower or keyword in full_context 
                   for keyword in info["keywords"]):
                mentioned_categories.add(category)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ —Ç–∏–ø—É –∑–∞–ø—Ä–æ—Å–∞
        required_categories = self._get_required_categories_by_type(query_type)
        
        # –ù–∞—Ö–æ–¥–∏–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        missing_categories = required_categories - mentioned_categories
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
        high_priority = []
        medium_priority = []
        low_priority = []
        
        for category in missing_categories:
            importance = info_categories.get(category, {}).get("importance", "–Ω–∏–∑–∫–∞—è")
            if importance == "–≤—ã—Å–æ–∫–∞—è":
                high_priority.append(category)
            elif importance == "—Å—Ä–µ–¥–Ω—è—è":
                medium_priority.append(category)
            else:
                low_priority.append(category)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
        missing.extend(high_priority)
        missing.extend(medium_priority[:2])  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Å—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        missing.extend(low_priority[:1])     # –¢–æ–ª—å–∫–æ –æ–¥–∏–Ω –Ω–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        
        return missing[:4]  # –ú–∞–∫—Å–∏–º—É–º 4 –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    
    def _determine_query_type(self, query: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤"""
        
        if any(word in query for word in ["–ø—Ä–æ–±–ª–µ–º–∞", "–æ—à–∏–±–∫–∞", "–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç", "—Å–ª–æ–º–∞–ª–æ—Å—å"]):
            return "problem"
        elif any(word in query for word in ["–±–æ–Ω—É—Å", "–±–∞–ª–ª—ã", "vlife", "–ª–æ—è–ª—å–Ω–æ—Å—Ç—å"]):
            return "bonus"
        elif any(word in query for word in ["–æ–ø–ª–∞—Ç–∞", "–∫–∞—Ä—Ç–∞", "–¥–µ–Ω—å–≥–∏", "–ø–ª–∞—Ç–µ–∂"]):
            return "payment"
        elif any(word in query for word in ["—Ç–æ–ø–ª–∏–≤–æ", "–±–µ–Ω–∑–∏–Ω", "–¥–∏–∑–µ–ª—å", "–∫–∞—á–µ—Å—Ç–≤–æ"]):
            return "fuel"
        elif any(word in query for word in ["–∫–∞–∫", "—á—Ç–æ", "–≥–¥–µ", "–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è"]):
            return "instruction"
        else:
            return "general"
    
    def _get_required_categories_by_type(self, query_type: str) -> Set[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ —Ç–∏–ø—É –∑–∞–ø—Ä–æ—Å–∞"""
        
        type_requirements = {
            "problem": {"location", "time", "details"},
            "bonus": {"location", "time", "contact", "payment_method"},
            "payment": {"amount", "time", "location", "payment_method"},
            "fuel": {"location", "time", "amount", "vehicle"},
            "instruction": {"details"},
            "general": {"details"}
        }
        
        return type_requirements.get(query_type, {"details"})
    
    @torch.inference_mode()
    def _generate_comprehensive_questions(self, user_query: str, analysis: Dict, missing_info: List[str]) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–º–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∑–Ω–∞–Ω–∏–π"""
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context_summary = analysis["full_context"][:1000]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        procedures = analysis.get("all_procedures", [])[:3]
        required_fields = analysis.get("all_required_fields", [])[:5]
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö
        source_info = analysis.get("source_breakdown", {})
        dominant_source = analysis.get("dominant_source", "hybrid")
        
        # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        prompt = f"""Human: –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: "{user_query}"

–ù–ê–ô–î–ï–ù–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø –ò–ó –í–°–ï–• –ò–°–¢–û–ß–ù–ò–ö–û–í:
{context_summary}

–ö–ª—é—á–µ–≤—ã–µ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã: {'; '.join(procedures) if procedures else '–ù–µ –Ω–∞–π–¥–µ–Ω—ã'}
–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è: {'; '.join(required_fields) if required_fields else '–ù–µ –Ω–∞–π–¥–µ–Ω—ã'}

–ê–ù–ê–õ–ò–ó –ò–°–¢–û–ß–ù–ò–ö–û–í:
- –î–æ–∫—É–º–µ–Ω—Ç—ã Qdrant: {source_info.get('qdrant_sources', 0)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {source_info.get('qdrant_confidence', 0):.2f})
- –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π Q&A: {source_info.get('qa_matches', 0)} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {source_info.get('qa_confidence', 0):.2f})
- –î–æ–º–∏–Ω–∏—Ä—É—é—â–∏–π –∏—Å—Ç–æ—á–Ω–∏–∫: {dominant_source}

–ù–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏: {', '.join(missing_info)}

–ó–ê–î–ê–ß–ê: –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π 2-3 –ö–û–ù–ö–†–ï–¢–ù–´–• —É—Ç–æ—á–Ω—è—é—â–∏—Ö –≤–æ–ø—Ä–æ—Å–∞, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–º–æ–≥—É—Ç –ø–æ–ª—É—á–∏—Ç—å –Ω–µ–¥–æ—Å—Ç–∞—é—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

–í–æ–ø—Ä–æ—Å—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å:
- –û—Å–Ω–æ–≤–∞–Ω—ã –Ω–∞ –†–ï–ê–õ–¨–ù–û–ô –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
- –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –∏ –ø—Ä–∞–∫—Ç–∏—á–Ω—ã–º–∏
- –£—á–∏—Ç—ã–≤–∞—é—â–∏–º–∏ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ò –æ–ø—ã—Ç –∏–∑ –±–∞–∑—ã Q&A
- –ü–æ–º–æ–≥–∞—é—â–∏–º–∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–æ—á–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:
1. [–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –≤–æ–ø—Ä–æ—Å]
2. [–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –≤–æ–ø—Ä–æ—Å]  
3. [–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –≤–æ–ø—Ä–æ—Å]

Assistant:"""

        try:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=400,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–ª—è –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
                do_sample=True,
                temperature=0.2,    # –°–Ω–∏–∂–∞–µ–º –¥–ª—è –±–æ–ª–µ–µ —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
                repetition_penalty=1.3,
                pad_token_id=self.tokenizer.eos_token_id
            )
            
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã
            if "Assistant:" in response:
                questions_text = response.split("Assistant:")[-1].strip()
            else:
                questions_text = response.replace(prompt, "").strip()
            
            # –ü–∞—Ä—Å–∏–º –≤–æ–ø—Ä–æ—Å—ã
            questions = self._parse_generated_questions(questions_text)
            
            # –ï—Å–ª–∏ LLM –Ω–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª –≤–æ–ø—Ä–æ—Å—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback
            if not questions:
                questions = self._generate_comprehensive_fallback_questions(
                    user_query, missing_info, analysis
                )
            
            return questions
            
        except Exception as e:
            log.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤: {e}")
            return self._generate_comprehensive_fallback_questions(
                user_query, missing_info, analysis
            )
    
    def _parse_generated_questions(self, text: str) -> List[str]:
        """–ü–∞—Ä—Å–∏—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM"""
        questions = []
        
        # –ò—â–µ–º –Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã
        patterns = [
            r'\d+\.\s*([^?\n]+\?)',  # "1. –í–æ–ø—Ä–æ—Å?"
            r'\d+\)\s*([^?\n]+\?)',  # "1) –í–æ–ø—Ä–æ—Å?"
            r'[‚Ä¢-]\s*([^?\n]+\?)',   # "‚Ä¢ –í–æ–ø—Ä–æ—Å?" –∏–ª–∏ "- –í–æ–ø—Ä–æ—Å?"
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                clean_question = match.strip()
                if len(clean_question) > 15 and clean_question not in questions:
                    questions.append(clean_question)
        
        # –ï—Å–ª–∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏, –∏—â–µ–º –ª—é–±—ã–µ –≤–æ–ø—Ä–æ—Å—ã
        if not questions:
            question_sentences = re.findall(r'[^.!?\n]+\?', text)
            for q in question_sentences:
                clean_q = q.strip()
                if len(clean_q) > 15 and clean_q not in questions:
                    questions.append(clean_q)
        
        return questions[:3]  # –ú–∞–∫—Å–∏–º—É–º 3 –≤–æ–ø—Ä–æ—Å–∞
    
    def _generate_comprehensive_fallback_questions(self, user_query: str, missing_info: List[str], analysis: Dict) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç fallback –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        
        query_type = self._determine_query_type(user_query.lower())
        dominant_source = analysis.get("dominant_source", "hybrid")
        
        # –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –ø–æ —Ç–∏–ø—É –∑–∞–ø—Ä–æ—Å–∞ –∏ –∏—Å—Ç–æ—á–Ω–∏–∫—É
        specialized_questions = {
            "problem": {
                "qdrant": [
                    "–ù–∞ –∫–∞–∫–æ–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ –ê–ó–° –≤–æ–∑–Ω–∏–∫–ª–∞ –ø—Ä–æ–±–ª–µ–º–∞? –°–æ–≥–ª–∞—Å–Ω–æ –Ω–∞—à–∏–º –ø—Ä–æ—Ü–µ–¥—É—Ä–∞–º, –Ω–æ–º–µ—Ä —Å—Ç–∞–Ω—Ü–∏–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏.",
                    "–ö–æ–≥–¥–∞ –∏–º–µ–Ω–Ω–æ —ç—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ? –≠—Ç–æ –ø–æ–º–æ–∂–µ—Ç –Ω–∞–π—Ç–∏ –æ–ø–µ—Ä–∞—Ü–∏—é –≤ —Å–∏—Å—Ç–µ–º–µ –∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø—Ä–∏—á–∏–Ω—É —Å–±–æ—è.",
                    "–û–ø–∏—à–∏—Ç–µ –ø–æ–¥—Ä–æ–±–Ω–µ–µ —Å–∏–º–ø—Ç–æ–º—ã –ø—Ä–æ–±–ª–µ–º—ã - —ç—Ç–æ –ø–æ–º–æ–∂–µ—Ç –ø—Ä–∏–º–µ–Ω–∏—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –ø—Ä–æ—Ü–µ–¥—É—Ä—É —Ä–µ—à–µ–Ω–∏—è."
                ],
                "qa_dataset": [
                    "–°—É–¥—è –ø–æ –ø–æ—Ö–æ–∂–∏–º —Å–ª—É—á–∞—è–º, –≤–∞–∂–Ω–æ –∑–Ω–∞—Ç—å: –Ω–∞ –∫–∞–∫–æ–π –ê–ó–° —ç—Ç–æ —Å–ª—É—á–∏–ª–æ—Å—å –∏ –∫–æ–≥–¥–∞?",
                    "–í –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã—Ö —Å–∏—Ç—É–∞—Ü–∏—è—Ö –ø–æ–º–æ–≥–∞–ª–æ —É—Ç–æ—á–Ω–µ–Ω–∏–µ: –∫–∞–∫–∏–µ –∏–º–µ–Ω–Ω–æ –¥–µ–π—Å—Ç–≤–∏—è –≤—ã –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞–ª–∏?",
                    "–ò—Å—Ö–æ–¥—è –∏–∑ –æ–ø—ã—Ç–∞ —Ä–µ—à–µ–Ω–∏—è –ø–æ–¥–æ–±–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º: –µ—Å—Ç—å –ª–∏ —É –≤–∞—Å –Ω–æ–º–µ—Ä –æ–ø–µ—Ä–∞—Ü–∏–∏ –∏–ª–∏ —á–µ–∫–∞?"
                ],
                "hybrid": [
                    "–î–ª—è —Ç–æ—á–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –Ω—É–∂–Ω—ã –¥–µ—Ç–∞–ª–∏: –Ω–æ–º–µ—Ä –ê–ó–°, –¥–∞—Ç–∞/–≤—Ä–µ–º—è –∏ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã.",
                    "–ß—Ç–æ–±—ã –ø—Ä–∏–º–µ–Ω–∏—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –ø—Ä–æ—Ü–µ–¥—É—Ä—É —Ä–µ—à–µ–Ω–∏—è: –æ–ø–∏—à–∏—Ç–µ, —á—Ç–æ –∏–º–µ–Ω–Ω–æ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç?",
                    "–ù–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞—à–µ–≥–æ –æ–ø—ã—Ç–∞: –∫–∞–∫–∏–µ –¥–µ–π—Å—Ç–≤–∏—è –≤—ã —É–∂–µ –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞–ª–∏ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è?"
                ]
            },
            "bonus": {
                "qdrant": [
                    "–°–æ–≥–ª–∞—Å–Ω–æ —Ä–µ–≥–ª–∞–º–µ–Ω—Ç—É Vlife: –Ω–∞ –∫–∞–∫–æ–π –ê–ó–° –∏ –∫–æ–≥–¥–∞ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏–ª–∞ –∑–∞–ø—Ä–∞–≤–∫–∞?",
                    "–î–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞—á–∏—Å–ª–µ–Ω–∏—è –±–æ–Ω—É—Å–æ–≤: –∫–∞–∫–∞—è –±—ã–ª–∞ —Å—É–º–º–∞ –ø–æ–∫—É–ø–∫–∏ –∏ —Å–ø–æ—Å–æ–± –æ–ø–ª–∞—Ç—ã?",
                    "–ü–æ –ø—Ä–æ—Ü–µ–¥—É—Ä–µ –û–ö–ö–° –Ω—É–∂–Ω–æ –∑–Ω–∞—Ç—å: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –ª–∏ –≤—ã –∫–∞—Ä—Ç—É –ª–æ—è–ª—å–Ω–æ—Å—Ç–∏ –∏–ª–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ?"
                ],
                "qa_dataset": [
                    "–í –ø–æ—Ö–æ–∂–∏—Ö —Å–ª—É—á–∞—è—Ö –ø–æ–º–æ–≥–∞–ª–æ: –Ω–æ–º–µ—Ä –ê–ó–°, –¥–∞—Ç–∞ –∑–∞–ø—Ä–∞–≤–∫–∏ –∏ —Å—É–º–º–∞ –æ–ø–µ—Ä–∞—Ü–∏–∏.",
                    "–û–ø—ã—Ç –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å: –∫–∞–∫ –∏–º–µ–Ω–Ω–æ –≤—ã –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–ª–∏—Å—å –≤ —Å–∏—Å—Ç–µ–º–µ Vlife?",
                    "–î–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è: –µ—Å—Ç—å –ª–∏ —É –≤–∞—Å —á–µ–∫ –∏–ª–∏ –¥–∞–Ω–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏?"
                ],
                "hybrid": [
                    "–î–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã —Å –±–æ–Ω—É—Å–∞–º–∏ Vlife: –Ω–æ–º–µ—Ä –ê–ó–°, –¥–∞—Ç–∞/–≤—Ä–µ–º—è –∏ —Å—É–º–º–∞ –∑–∞–ø—Ä–∞–≤–∫–∏?",
                    "–ö–∞–∫ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏–ª–∞ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è: —á–µ—Ä–µ–∑ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ, –∫–∞—Ä—Ç—É –∏–ª–∏ –Ω–æ–º–µ—Ä —Ç–µ–ª–µ—Ñ–æ–Ω–∞?",
                    "–ü—Ä–æ–≤–µ—Ä—è–ª—Å—è –ª–∏ –±–∞–ª–∞–Ω—Å –±–æ–Ω—É—Å–æ–≤ —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –∑–∞–ø—Ä–∞–≤–∫–∏?"
                ]
            },
            "payment": {
                "qdrant": [
                    "–ü–æ –ø—Ä–æ—Ü–µ–¥—É—Ä–µ –≤–æ–∑–≤—Ä–∞—Ç–∞ —Å—Ä–µ–¥—Å—Ç–≤: –Ω–æ–º–µ—Ä –ê–ó–°, —Ç–æ—á–Ω–æ–µ –≤—Ä–µ–º—è –∏ —Å—É–º–º–∞ –æ–ø–µ—Ä–∞—Ü–∏–∏?",
                    "–î–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤ —Å–∏—Å—Ç–µ–º–µ: –ø–æ—Å–ª–µ–¥–Ω–∏–µ 4 —Ü–∏—Ñ—Ä—ã –∫–∞—Ä—Ç—ã –∏ —Ç–∏–ø –æ–ø–µ—Ä–∞—Ü–∏–∏?",
                    "–°–æ–≥–ª–∞—Å–Ω–æ —Ä–µ–≥–ª–∞–º–µ–Ω—Ç—É: –µ—Å—Ç—å –ª–∏ –Ω–æ–º–µ—Ä –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –∏–ª–∏ —á–µ–∫–∞?"
                ],
                "qa_dataset": [
                    "–í –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã—Ö —Å–ª—É—á–∞—è—Ö –≤–∞–∂–Ω–æ: –∫–∞–∫–æ–π –∏–º–µ–Ω–Ω–æ –±—ã–ª —Ç–∏–ø –æ—à–∏–±–∫–∏ –ø–ª–∞—Ç–µ–∂–∞?",
                    "–û–ø—ã—Ç –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç: –ø–æ–º–æ–≥–∞–µ—Ç –Ω–æ–º–µ—Ä —Ç–µ—Ä–º–∏–Ω–∞–ª–∞ –∏ —Ç–æ—á–Ω–æ–µ –≤—Ä–µ–º—è –æ–ø–µ—Ä–∞—Ü–∏–∏.",
                    "–î–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –≤–æ–∑–≤—Ä–∞—Ç–∞: –∫–∞–∫–æ–π —Å–ø–æ—Å–æ–± –æ–ø–ª–∞—Ç—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è?"
                ],
                "hybrid": [
                    "–î–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã —Å –æ–ø–ª–∞—Ç–æ–π: –Ω–æ–º–µ—Ä –ê–ó–°, –≤—Ä–µ–º—è –∏ —Å—É–º–º–∞ –æ–ø–µ—Ä–∞—Ü–∏–∏?",
                    "–ß—Ç–æ –∏–º–µ–Ω–Ω–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ: –∫–∞—Ä—Ç–∞ –Ω–µ –ø—Ä–æ—à–ª–∞, –¥–µ–Ω—å–≥–∏ —Å–ø–∏—Å–∞–ª–∏—Å—å –±–µ–∑ —Ç–æ–ø–ª–∏–≤–∞, –∏–ª–∏ –æ—à–∏–±–∫–∞ —Ç–µ—Ä–º–∏–Ω–∞–ª–∞?",
                    "–ï—Å—Ç—å –ª–∏ —É –≤–∞—Å SMS –æ —Å–ø–∏—Å–∞–Ω–∏–∏ –∏–ª–∏ –Ω–æ–º–µ—Ä –æ–ø–µ—Ä–∞—Ü–∏–∏ –≤ –±–∞–Ω–∫–æ–≤—Å–∫–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏?"
                ]
            }
        }
        
        # –ë–∞–∑–æ–≤—ã–µ fallback –≤–æ–ø—Ä–æ—Å—ã
        fallback_mapping = {
            "location": "–ù–∞ –∫–∞–∫–æ–π –ê–ó–° —ç—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ? –£–∫–∞–∂–∏—Ç–µ –Ω–æ–º–µ—Ä —Å—Ç–∞–Ω—Ü–∏–∏ –∏–ª–∏ –∞–¥—Ä–µ—Å.",
            "time": "–ö–æ–≥–¥–∞ –∏–º–µ–Ω–Ω–æ —ç—Ç–æ —Å–ª—É—á–∏–ª–æ—Å—å? –ù—É–∂–Ω—ã –¥–∞—Ç–∞ –∏ –ø—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è.",
            "amount": "–ö–∞–∫–∞—è –±—ã–ª–∞ —Å—É–º–º–∞ –æ–ø–µ—Ä–∞—Ü–∏–∏ –∏–ª–∏ –æ–±—ä–µ–º –∑–∞–ø—Ä–∞–≤–∫–∏?",
            "contact": "–ö–∞–∫–æ–π –Ω–æ–º–µ—Ä —Ç–µ–ª–µ—Ñ–æ–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ –¥–ª—è —Å–≤—è–∑–∏ —Å –Ω–∞–º–∏?",
            "details": "–û–ø–∏—à–∏—Ç–µ –ø–æ–¥—Ä–æ–±–Ω–µ–µ, —á—Ç–æ –∏–º–µ–Ω–Ω–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ –∏ –∫–∞–∫ —ç—Ç–æ –ø—Ä–æ—è–≤–ª—è–µ—Ç—Å—è.",
            "payment_method": "–ö–∞–∫–∏–º —Å–ø–æ—Å–æ–±–æ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏–ª–∞—Å—å –æ–ø–ª–∞—Ç–∞: –∫–∞—Ä—Ç–∞, –Ω–∞–ª–∏—á–Ω—ã–µ –∏–ª–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ?",
            "document": "–ï—Å—Ç—å –ª–∏ —É –≤–∞—Å –Ω–æ–º–µ—Ä —á–µ–∫–∞, –æ–ø–µ—Ä–∞—Ü–∏–∏ –∏–ª–∏ –¥—Ä—É–≥–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã?",
            "vehicle": "–ö–∞–∫–æ–π –∞–≤—Ç–æ–º–æ–±–∏–ª—å: –º–∞—Ä–∫–∞, –º–æ–¥–µ–ª—å, –≥–æ—Å. –Ω–æ–º–µ—Ä?"
        }
        
        questions = []
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã
        if query_type in specialized_questions:
            source_questions = specialized_questions[query_type].get(dominant_source, [])
            if source_questions:
                questions.extend(source_questions[:2])
        
        # –î–æ–ø–æ–ª–Ω—è–µ–º –±–∞–∑–æ–≤—ã–º–∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–µ–¥–æ—Å—Ç–∞—é—â–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        for info_type in missing_info:
            if info_type in fallback_mapping and len(questions) < 3:
                questions.append(fallback_mapping[info_type])
        
        # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –º–∞–ª–æ –≤–æ–ø—Ä–æ—Å–æ–≤, –¥–æ–±–∞–≤–ª—è–µ–º —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ
        if len(questions) < 2:
            universal_questions = [
                "–†–∞—Å—Å–∫–∞–∂–∏—Ç–µ –ø–æ–¥—Ä–æ–±–Ω–µ–µ –æ –≤–∞—à–µ–π —Å–∏—Ç—É–∞—Ü–∏–∏ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã.",
                "–ö–∞–∫–∏–µ –¥–µ–π—Å—Ç–≤–∏—è –≤—ã —É–∂–µ –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞–ª–∏ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞?",
                "–ï—Å—Ç—å –ª–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–µ—Ç–∞–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –ø–æ–º–æ—á—å –≤ —Ä–µ—à–µ–Ω–∏–∏?"
            ]
            for q in universal_questions:
                if len(questions) < 3:
                    questions.append(q)
        
        return questions[:3]

    def should_ask_comprehensive_questions(self, analysis_result: Dict) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –∑–∞–¥–∞–≤–∞—Ç—å —É—Ç–æ—á–Ω—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        
        if analysis_result["status"] != "comprehensive_analysis_complete":
            return False
        
        # –§–∞–∫—Ç–æ—Ä—ã –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏—è
        has_missing_info = len(analysis_result.get("missing_info", [])) > 0
        low_confidence = analysis_result.get("confidence", 0) < 0.7
        multiple_sources = analysis_result.get("sources", {}).get("total_sources", 0) > 1
        has_procedures = len(analysis_result.get("combined_context", "")) > 100
        
        # –ê–Ω–∞–ª–∏–∑ –¥–æ–º–∏–Ω–∏—Ä—É—é—â–µ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        dominant_source = analysis_result.get("sources", {})
        qdrant_strong = dominant_source.get("qdrant_sources", 0) >= 3
        qa_strong = dominant_source.get("qa_matches", 0) >= 2
        
        # –õ–æ–≥–∏–∫–∞ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏—è
        should_ask = (
            has_missing_info or                           # –ï—Å—Ç—å –Ω–µ–¥–æ—Å—Ç–∞—é—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            (low_confidence and has_procedures) or       # –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –ø—Ä–æ—Ü–µ–¥—É—Ä
            (multiple_sources and not (qdrant_strong or qa_strong))  # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–ª–∞–±—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        )
        
        log.info(f"ü§î –ê–Ω–∞–ª–∏–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤–æ–ø—Ä–æ—Å–æ–≤: missing_info={has_missing_info}, "
                f"confidence={analysis_result.get('confidence', 0):.2f}, "
                f"should_ask={should_ask}")
        
        return should_ask

    def enhance_query_with_comprehensive_answers(self, original_query: str, user_answers: str) -> str:
        """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å —Å –æ—Ç–≤–µ—Ç–∞–º–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è"""
        
        enhanced = f"""–ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {original_query}

–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_answers}

–û–ë–™–ï–î–ò–ù–ï–ù–ù–´–ô –ó–ê–ü–†–û–°: {original_query}

–ö–û–ù–¢–ï–ö–°–¢ –î–ò–ê–õ–û–ì–ê:
–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏–ª –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–µ—Ç–∞–ª–∏: {user_answers}

–ò—Å–ø–æ–ª—å–∑—É–π –≤—Å—é —ç—Ç—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–æ—á–Ω–æ–≥–æ –∏ –ø–æ–ª–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞."""
        
        return enhanced

# –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –æ—Å–Ω–æ–≤–Ω–æ–π —Å–∏—Å—Ç–µ–º–æ–π RAG
class EnhancedIntelligentRAGSystem:
    """–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ —É–º–Ω–æ–≥–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π RAG —Å–∏—Å—Ç–µ–º–æ–π"""
    
    def __init__(self, original_generate_func, model, tokenizer):
        self.original_generate = original_generate_func
        self.assistant = EnhancedIntelligentProactiveAssistant(model, tokenizer)
        self.active_sessions = {}  # –•—Ä–∞–Ω–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å–µ—Å—Å–∏–π –¥–∏–∞–ª–æ–≥–∞
    
    def enhanced_generate(self, query: str, session_id: str = "default") -> Dict:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∞–∫—Ç–∏–≤–Ω–∞—è —Å–µ—Å—Å–∏—è
        if session_id in self.active_sessions:
            session = self.active_sessions[session_id]
            
            # –≠—Ç–æ –æ—Ç–≤–µ—Ç –Ω–∞ —É—Ç–æ—á–Ω—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã
            if session.get("awaiting_response"):
                enhanced_query = self.assistant.enhance_query_with_comprehensive_answers(
                    session["original_query"], query
                )
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
                final_response = self.original_generate(enhanced_query)
                
                # –û—á–∏—â–∞–µ–º —Å–µ—Å—Å–∏—é
                del self.active_sessions[session_id]
                
                return {
                    "type": "final_answer",
                    "content": final_response,
                    "session_complete": True,
                    "analysis_used": session.get("analysis", {})
                }
        
        # –ù–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å - –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        analysis = self.assistant.analyze_comprehensive_knowledge(query)
        
        if self.assistant.should_ask_comprehensive_questions(analysis):
            # –ù—É–∂–Ω—ã —É—Ç–æ—á–Ω—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã
            questions = analysis.get("questions", [])
            
            if questions:
                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Å–µ—Å—Å–∏—é
                self.active_sessions[session_id] = {
                    "original_query": query,
                    "analysis": analysis,
                    "awaiting_response": True,
                    "questions": questions
                }
                
                # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                formatted_questions = self._format_comprehensive_questions(questions, analysis)
                
                return {
                    "type": "clarifying_questions",
                    "content": formatted_questions,
                    "session_active": True,
                    "questions": questions,
                    "analysis": analysis
                }
        
        # –ù–µ –Ω—É–∂–Ω—ã —É—Ç–æ—á–Ω–µ–Ω–∏—è - –æ—Ç–≤–µ—á–∞–µ–º —Å—Ä–∞–∑—É
        response = self.original_generate(query)
        return {
            "type": "direct_answer",
            "content": response,
            "session_complete": True,
            "analysis": analysis
        }
    
    def _format_comprehensive_questions(self, questions: List[str], analysis: Dict) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –≤–æ–ø—Ä–æ—Å—ã —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–º –∞–Ω–∞–ª–∏–∑–µ"""
        
        confidence = analysis.get("confidence", 0)
        sources = analysis.get("sources", {})
        total_sources = sources.get("total_sources", 0)
        qdrant_sources = sources.get("qdrant_sources", 0)
        qa_matches = sources.get("qa_matches", 0)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–∫–æ–Ω–∫–∏ –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        sources_info = []
        if qdrant_sources > 0:
            sources_info.append(f"üìö {qdrant_sources} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        if qa_matches > 0:
            sources_info.append(f"üí° {qa_matches} Q&A —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π")
        
        sources_text = " + ".join(sources_info) if sources_info else "–æ–±—â–∞—è –±–∞–∑–∞"
        
        intro = f"""üîç **–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –≤–∞—à—É —Å–∏—Ç—É–∞—Ü–∏—é –ø–æ –≤—Å–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º...**

–ù–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏: **{sources_text}** (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.0%})

üéØ **–î–ª—è —Ç–æ—á–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è —É—Ç–æ—á–Ω–∏—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–µ—Ç–∞–ª–µ–π:**"""
        
        formatted_questions = intro + "\n\n"
        
        for i, question in enumerate(questions, 1):
            formatted_questions += f"**{i}.** {question}\n\n"
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö
        if qdrant_sources > 0 and qa_matches > 0:
            source_note = "üí¨ *–û—Ç–≤–µ—Ç –±—É–¥–µ—Ç –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –∏ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω–æ–º –æ–ø—ã—Ç–µ —Ä–µ—à–µ–Ω–∏—è –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤*"
        elif qdrant_sources > 0:
            source_note = "üìã *–û—Ç–≤–µ—Ç –±—É–¥–µ—Ç –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–æ—Ü–µ–¥—É—Ä–∞—Ö –∏ —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–∞—Ö*"
        elif qa_matches > 0:
            source_note = "üéØ *–û—Ç–≤–µ—Ç –±—É–¥–µ—Ç –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –æ–ø—ã—Ç–µ —Ä–µ—à–µ–Ω–∏—è –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã—Ö —Å–∏—Ç—É–∞—Ü–∏–π*"
        else:
            source_note = ""
        
        if source_note:
            formatted_questions += f"\n{source_note}\n\n"
        
        formatted_questions += "üí¨ **–û—Ç–≤–µ—Ç—å—Ç–µ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –≤—ã—à–µ, –∏ —è –¥–∞–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ!**"
        
        return formatted_questions

# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Å–∏—Å—Ç–µ–º–æ–π
def create_enhanced_intelligent_rag_system(rag_module):
    """–°–æ–∑–¥–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—É—é —É–º–Ω—É—é RAG —Å–∏—Å—Ç–µ–º—É –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π"""
    
    try:
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑ rag_chat
        from rag_chat import model, tokenizer
        
        # –°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é —É–º–Ω—É—é —Å–∏—Å—Ç–µ–º—É
        intelligent_system = EnhancedIntelligentRAGSystem(
            rag_module.generate, model, tokenizer
        )
        
        return intelligent_system
        
    except Exception as e:
        log.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —É–ª—É—á—à–µ–Ω–Ω–æ–π —É–º–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã: {e}")
        return None

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —É–º–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã
intelligent_rag = None

def init_intelligent_system(rag_module):
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É–ª—É—á—à–µ–Ω–Ω–æ–π —É–º–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã"""
    global intelligent_rag
    
    if intelligent_rag is None:
        intelligent_rag = create_enhanced_intelligent_rag_system(rag_module)
        if intelligent_rag:
            log.info("‚úÖ –£–ª—É—á—à–µ–Ω–Ω–∞—è —É–º–Ω–∞—è RAG —Å–∏—Å—Ç–µ–º–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
            log.info(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ Q&A –∑–∞–ø–∏—Å–µ–π: {len(intelligent_rag.assistant.qa_dataset)}")
            return True
        else:
            log.error("‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —É–ª—É—á—à–µ–Ω–Ω–æ–π —É–º–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã")
            return False
    return True

def intelligent_generate(query: str, session_id: str = "default") -> Dict:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–π —É–º–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
    global intelligent_rag
    
    if intelligent_rag is None:
        # Fallback –∫ –æ–±—ã—á–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        import rag_chat
        return {
            "type": "direct_answer", 
            "content": rag_chat.generate(query),
            "session_complete": True
        }
    
    return intelligent_rag.enhanced_generate(query, session_id)